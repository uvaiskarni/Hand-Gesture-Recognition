{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsystem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to get the landmark points and predict the gesture type using subsystem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from sklearn import svm\n",
    "from skimage import color\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np \n",
    "import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This snippet allows to list all the frames saved to be used for training SVM classifier to differentiate palm and dorsal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "# Read the images dataset and prepare labels for Dorsal and Palm classifier\n",
    "fnames = os.listdir('Images_data')\n",
    "labels = []\n",
    "for j in range(len(fnames)):\n",
    "    val = ''\n",
    "    temp = fnames[j].split('_')\n",
    "    if 'palm' in temp:\n",
    "        labels.append('palm')\n",
    "    else:\n",
    "        labels.append('dorsal')\n",
    "print(len(fnames))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Images to extract the HOG features \n",
    "\n",
    "hog_features = []\n",
    "img_length = 80\n",
    "for i in fnames:\n",
    "    file_name = 'Images_data' + '//' + i\n",
    "    img = imread(file_name)\n",
    "    #creating hog features  \n",
    "    re_img = resize(img, (128,64))\n",
    "    fd, hog_image = hog(re_img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                    cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "    hog_features.append(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the vectors and shuffle the data\n",
    "\n",
    "new_labels =  np.array(labels).reshape(len(labels),1)\n",
    "new_hog_features = np.array(hog_features)\n",
    "data_frame = np.hstack((new_hog_features,new_labels))\n",
    "np.random.shuffle(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit the dataset into train and test set\n",
    "\n",
    "percentage = 80\n",
    "partition = int(len(hog_features)*percentage/100)\n",
    "x_train, x_test = data_frame[:partition,:-1],  data_frame[partition:,:-1]\n",
    "y_train, y_test = data_frame[:partition,-1:].ravel() , data_frame[partition:,-1:].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the SVM classifier model and fit the data to train\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Palmdorsal_class_model/Palmdorsal_Model']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the SVM model\n",
    "\n",
    "joblib.dump(clf, \"Palmdorsal_class_model/Palmdorsal_Model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      dorsal       1.00      1.00      1.00       246\n",
      "        palm       1.00      1.00      1.00       234\n",
      "\n",
      "    accuracy                           1.00       480\n",
      "   macro avg       1.00      1.00      1.00       480\n",
      "weighted avg       1.00      1.00      1.00       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test set\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"Accuracy: \"+str(accuracy_score(y_test, y_pred)))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This snippet lists all video files for which the gestures are to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training images for HOG features\n",
    "\n",
    "import os\n",
    "folder_loc = 'C:/Uppsala/Sem 2/Intelligent IS/Project/videos/'\n",
    "arr = os.listdir(r'C:/Uppsala/Sem 2/Intelligent IS/Project/videos/.')\n",
    "new_arr = []\n",
    "for i in range(len(arr)):\n",
    "    folder = folder_loc + arr[i] + '/.'\n",
    "    fnames = os.listdir(folder)\n",
    "    for j in range(len(fnames)):\n",
    "        temp = folder_loc + arr[i] + '/' + fnames[j]\n",
    "        new_arr.append(temp)\n",
    "k=0\n",
    "for i in range(len(new_arr)):\n",
    "    cap = cv2.VideoCapture(new_arr[i])\n",
    "    for j in range(40):\n",
    "        ret,image = cap.read() \n",
    "        lst = new_arr[i].split('/')\n",
    "        fname = lst[-1].split('.')[0] + '_' + str(k)\n",
    "        cv2.imwrite('Images_data/%s.png' % fname,image)\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['palm']\n"
     ]
    }
   ],
   "source": [
    "# Detect palm or dorsal type \n",
    "\n",
    "input_source = r\"C:\\Uppsala\\Sem 2\\Intelligent IS\\Project\\videos\\564\\three_fingers_palm.webm\"\n",
    "cap = cv2.VideoCapture(input_source)\n",
    "hasFrame, frame = cap.read()\n",
    "re_img = resize(frame, (128,64))\n",
    "fd, hog_image = hog(re_img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                    cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "feature_pred = fd.reshape(1,len(fd))\n",
    "res = clf.predict(feature_pred)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This snippet is used to find the landmark points and draw them on the frames. Finally predict the gesture type using the saved model from susbsystem 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/fist_palm.webm\n",
      "0\n",
      "Annotated_2/fist_palm_1.mp4\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/fist_palm.webm\n",
      "WARNING:tensorflow:From c:\\users\\prave\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From c:\\users\\prave\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\prave\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Time Taken for frame = 5.595217704772949\n",
      "1\n",
      "Time Taken for frame = 2.323603868484497\n",
      "2\n",
      "Time Taken for frame = 2.3979580402374268\n",
      "3\n",
      "Time Taken for frame = 2.5100085735321045\n",
      "4\n",
      "Time Taken for frame = 2.6271724700927734\n",
      "5\n",
      "Time Taken for frame = 5.997422933578491\n",
      "6\n",
      "Time Taken for frame = 6.4571373462677\n",
      "7\n",
      "Time Taken for frame = 6.818955421447754\n",
      "8\n",
      "Time Taken for frame = 6.493856191635132\n",
      "9\n",
      "Time Taken for frame = 6.948557376861572\n",
      "10\n",
      "Time Taken for frame = 7.110571622848511\n",
      "11\n",
      "Time Taken for frame = 7.452145338058472\n",
      "12\n",
      "Time Taken for frame = 7.593806505203247\n",
      "13\n",
      "Time Taken for frame = 7.776116371154785\n",
      "14\n",
      "Time Taken for frame = 7.803938388824463\n",
      "15\n",
      "Time Taken for frame = 8.100454092025757\n",
      "16\n",
      "Time Taken for frame = 8.241311073303223\n",
      "17\n",
      "Time Taken for frame = 8.46025824546814\n",
      "18\n",
      "Time Taken for frame = 8.606940507888794\n",
      "19\n",
      "Time Taken for frame = 8.879628658294678\n",
      "20\n",
      "Time Taken for frame = 9.066803693771362\n",
      "21\n",
      "Time Taken for frame = 9.359199047088623\n",
      "22\n",
      "Time Taken for frame = 9.480819702148438\n",
      "23\n",
      "Time Taken for frame = 9.526289463043213\n",
      "24\n",
      "Time Taken for frame = 9.765692234039307\n",
      "25\n",
      "Time Taken for frame = 9.965574264526367\n",
      "26\n",
      "Time Taken for frame = 10.282714605331421\n",
      "27\n",
      "Time Taken for frame = 10.292597532272339\n",
      "28\n",
      "Time Taken for frame = 10.386513471603394\n",
      "29\n",
      "Time Taken for frame = 10.573988437652588\n",
      "30\n",
      "Time Taken for frame = 10.905029773712158\n",
      "31\n",
      "Time Taken for frame = 11.07663369178772\n",
      "32\n",
      "Time Taken for frame = 11.11566424369812\n",
      "33\n",
      "Time Taken for frame = 11.629330158233643\n",
      "34\n",
      "Time Taken for frame = 11.569843292236328\n",
      "35\n",
      "Time Taken for frame = 12.13842225074768\n",
      "36\n",
      "Time Taken for frame = 11.869697570800781\n",
      "37\n",
      "Time Taken for frame = 12.210004806518555\n",
      "38\n",
      "Time Taken for frame = 12.403239250183105\n",
      "39\n",
      "Time Taken for frame = 12.497867107391357\n",
      "40\n",
      "Time Taken for frame = 13.594880819320679\n",
      "41\n",
      "Time Taken for frame = 13.08616852760315\n",
      "42\n",
      "Time Taken for frame = 12.887905597686768\n",
      "43\n",
      "Time Taken for frame = 15.275398015975952\n",
      "44\n",
      "Time Taken for frame = 15.237320899963379\n",
      "45\n",
      "Time Taken for frame = 15.387955904006958\n",
      "46\n",
      "Time Taken for frame = 15.742079973220825\n",
      "47\n",
      "Time Taken for frame = 15.722479104995728\n",
      "48\n",
      "Time Taken for frame = 16.05252432823181\n",
      "49\n",
      "Time Taken for frame = 16.114567518234253\n",
      "50\n",
      "Time Taken for frame = 16.295920372009277\n",
      "51\n",
      "Time Taken for frame = 16.495757818222046\n",
      "52\n",
      "Time Taken for frame = 16.58847665786743\n",
      "53\n",
      "Time Taken for frame = 16.920081853866577\n",
      "54\n",
      "Time Taken for frame = 17.143352508544922\n",
      "55\n",
      "Time Taken for frame = 17.253116369247437\n",
      "56\n",
      "Time Taken for frame = 17.64573383331299\n",
      "57\n",
      "Time Taken for frame = 17.850579500198364\n",
      "58\n",
      "Time Taken for frame = 17.614440441131592\n",
      "59\n",
      "Time Taken for frame = 17.957597017288208\n",
      "60\n",
      "Time Taken for frame = 18.904006481170654\n",
      "61\n",
      "Time Taken for frame = 18.409873962402344\n",
      "62\n",
      "Time Taken for frame = 18.630536317825317\n",
      "63\n",
      "Time Taken for frame = 18.979038953781128\n",
      "64\n",
      "Time Taken for frame = 19.11319637298584\n",
      "65\n",
      "Time Taken for frame = 19.757150888442993\n",
      "66\n",
      "Time Taken for frame = 19.635639667510986\n",
      "67\n",
      "Time Taken for frame = 19.82267475128174\n",
      "68\n",
      "Time Taken for frame = 19.89080023765564\n",
      "69\n",
      "Time Taken for frame = 20.037898540496826\n",
      "70\n",
      "Time Taken for frame = 20.580730676651\n",
      "71\n",
      "Time Taken for frame = 20.580374717712402\n",
      "72\n",
      "Time Taken for frame = 20.97030782699585\n",
      "73\n",
      "Time Taken for frame = 21.020560264587402\n",
      "74\n",
      "Time Taken for frame = 21.20909285545349\n",
      "75\n",
      "Time Taken for frame = 21.572514295578003\n",
      "76\n",
      "Time Taken for frame = 21.67859983444214\n",
      "77\n",
      "Time Taken for frame = 21.95811700820923\n",
      "78\n",
      "Time Taken for frame = 21.889482021331787\n",
      "79\n",
      "Time Taken for frame = 22.33028483390808\n",
      "80\n",
      "Time Taken for frame = 22.129636764526367\n",
      "81\n",
      "Time Taken for frame = 20.99654245376587\n",
      "82\n",
      "Time Taken for frame = 21.16684579849243\n",
      "83\n",
      "Time Taken for frame = 21.426176071166992\n",
      "84\n",
      "Time Taken for frame = 21.719796419143677\n",
      "85\n",
      "Time Taken for frame = 21.818180322647095\n",
      "86\n",
      "Time Taken for frame = 21.880126953125\n",
      "87\n",
      "Time Taken for frame = 22.25455141067505\n",
      "88\n",
      "Time Taken for frame = 22.745561838150024\n",
      "89\n",
      "Time Taken for frame = 22.841585397720337\n",
      "90\n",
      "Time Taken for frame = 22.750404596328735\n",
      "91\n",
      "Time Taken for frame = 23.237239360809326\n",
      "92\n",
      "Time Taken for frame = 23.39833116531372\n",
      "93\n",
      "Time Taken for frame = 23.594244241714478\n",
      "94\n",
      "Time Taken for frame = 23.701037406921387\n",
      "95\n",
      "Time Taken for frame = 24.065490245819092\n",
      "96\n",
      "Time Taken for frame = 24.405203819274902\n",
      "97\n",
      "Time Taken for frame = 24.327362537384033\n",
      "98\n",
      "Time Taken for frame = 24.822603225708008\n",
      "99\n",
      "Time Taken for frame = 25.014102697372437\n",
      "100\n",
      "Time Taken for frame = 25.159833669662476\n",
      "101\n",
      "Time Taken for frame = 25.324509620666504\n",
      "102\n",
      "Time Taken for frame = 25.997561931610107\n",
      "103\n",
      "Time Taken for frame = 25.99779963493347\n",
      "104\n",
      "Time Taken for frame = 26.043919324874878\n",
      "105\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/159/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/159/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/159/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/159/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/159/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/159/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/294/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/294/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/294/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/294/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/294/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/294/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/441/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/441/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/441/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/441/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/441/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/441/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/564/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/564/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/564/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/564/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/564/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/564/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/576/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/576/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/576/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/576/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/576/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/576/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/609/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/609/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/609/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/609/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/609/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/609/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/666/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/666/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/666/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/666/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/666/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/666/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/711/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/711/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/711/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/711/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/711/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/711/three_fingers_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/723/fist_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/723/fist_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/723/open_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/723/open_palm.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/723/three_fingers_dorsal.webm\n",
      "C:/Uppsala/Sem 2/Intelligent IS/Project/videos/723/three_fingers_palm.webm\n"
     ]
    }
   ],
   "source": [
    "# Landmarks extraction and drawing the keypoints for the gestures\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os,pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "protoFile = \"pose_deploy.prototxt\"\n",
    "weightsFile = \"pose_iter_102000.caffemodel\"\n",
    "nPoints = 21\n",
    "POSE_PAIRS = [ [0,2],[2,3],[3,4],[0,5],[5,6],[6,7],[7,8],[0,9],[9,10],[10,11],[11,12],[0,13],[13,14],[14,15],[15,16],[0,17],[17,18],[18,19],[19,20] ]\n",
    "\n",
    "threshold = 0.2\n",
    "\n",
    "lfolder = r\"C:\\Uppsala\\Sem 2\\Intelligent IS\\Project\"\n",
    "scaler_filename = \"Subsystem2_model/minmaxscaler.save\"\n",
    "subsys2_model = \"Subsystem2_model/GestureRecogModel.tfl\"\n",
    "palm_dorsal_model = \"Palmdorsal_class_model/Palmdorsal_Model\"\n",
    "\n",
    "gesture_types = ['Fist dorsal','Fist palm','Open dorsal','Open palm','Three fingers dorsal','Three fingers palm']\n",
    "\n",
    "folder_loc = 'C:/Uppsala/Sem 2/Intelligent IS/Project/videos/'\n",
    "ved_dirs = os.listdir(r'C:/Uppsala/Sem 2/Intelligent IS/Project/videos/.')\n",
    "ved_file_names = []\n",
    "for i in range(len(ved_dirs)):\n",
    "    folder = folder_loc + ved_dirs[i] + '/.'\n",
    "    fnames = os.listdir(folder)\n",
    "    for j in range(len(fnames)):\n",
    "        temp = folder_loc + ved_dirs[i] + '/' + fnames[j]\n",
    "        ved_file_names.append(temp)\n",
    "        \n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "backend = cv2.CAP_ANY\n",
    "fourcc_code = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "fps = 24\n",
    "num_frames = 0\n",
    "for l in range(len(ved_file_names)):\n",
    "    print(ved_file_names[l])\n",
    "    if ved_file_names[l] != r'C:/Uppsala/Sem 2/Intelligent IS/Project/videos/102/fist_palm.webm':\n",
    "        continue\n",
    "    print(num_frames)\n",
    "    num_frames = 0\n",
    "    annot_file_name = ved_file_names[l].split('/')[-1].split('.')[0] + '_' + str(l) + '.mp4'  \n",
    "    annot_file_name = 'Annotated_2/' + annot_file_name\n",
    "    print(annot_file_name)\n",
    "    cap = cv2.VideoCapture(ved_file_names[l])\n",
    "\n",
    "    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_size = (int(frameWidth),int(frameHeight))\n",
    "\n",
    "    aspect_ratio = frameWidth/frameHeight\n",
    "\n",
    "    inHeight = 368\n",
    "    inWidth = int(((aspect_ratio*inHeight)*8)//8)\n",
    "    vid_writer = cv2.VideoWriter(annot_file_name, backend, fourcc_code, int(fps), frame_size)\n",
    "    hasFrame, frame = cap.read()\n",
    "    print(ved_file_names[l])\n",
    "    flag = 1\n",
    "    while 1:\n",
    "        t = time.time()  \n",
    "        hasFrame, frame = cap.read()      \n",
    "        if not hasFrame:\n",
    "            break        \n",
    "        frameCopy = np.copy(frame)\n",
    "        num_frames += 1    \n",
    "        # Use the above trained model for classifying the frame as \"Palm\" or \"Dorsal\"\n",
    "        \n",
    "        if flag:\n",
    "            re_img = resize(frame, (128,64))\n",
    "            fd, hog_image = hog(re_img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                            cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "            feature_pred = fd.reshape(1,len(fd))\n",
    "            palm_dorsal_clas = joblib.load(palm_dorsal_model)\n",
    "            region_type = palm_dorsal_clas.predict(feature_pred)\n",
    "            flag = 0\n",
    "            #print(region_type)    \n",
    "\n",
    "        inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                              (0, 0, 0), swapRB=False, crop=False)\n",
    "    \n",
    "        net.setInput(inpBlob)\n",
    "        output = net.forward()\n",
    "\n",
    "        # Empty list to store the detected keypoints\n",
    "        points = []\n",
    "        temp = []\n",
    "        for i in range(nPoints):\n",
    "            # confidence map of corresponding body's part.\n",
    "            probMap = output[0, i, :, :]\n",
    "            probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
    "\n",
    "            # Find global maxima of the probMap.\n",
    "            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "            if prob > threshold :\n",
    "                cv2.circle(frameCopy, (int(point[0]), int(point[1])), 6, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "                cv2.putText(frameCopy, \"{}\".format(i), (int(point[0]), int(point[1])), cv2.FONT_HERSHEY_SIMPLEX, .8, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "                # Add the point to the list if the probability is greater than the threshold\n",
    "                points.append((int(point[0]), int(point[1])))\n",
    "                temp.append(int(point[0]))\n",
    "                temp.append(int(point[1]))\n",
    "            else :\n",
    "                points.append(None)\n",
    "                temp.append(0)\n",
    "                temp.append(0)\n",
    "            \n",
    "            # Format the landmark data to be passed to subsystem 2 for predicting the gesture type        \n",
    "        del temp[2:4]\n",
    "        if region_type == 'palm':\n",
    "            lst = [0] * 40       #If palm type - then insert zeros for the dorsal keypoints\n",
    "            temp.extend(lst)\n",
    "        else:\n",
    "            lst = [0] * 40       #If dorsal type - then insert zeros for the palm keypoints\n",
    "            lst.extend(temp)     \n",
    "            temp = lst\n",
    "    \n",
    "        # Draw Skeleton\n",
    "        for pair in POSE_PAIRS:\n",
    "            partA = pair[0]\n",
    "            partB = pair[1]\n",
    "            if points[partA] and points[partB]:\n",
    "                cv2.line(frame, points[partA], points[partB], (0, 255, 255), 2, lineType=cv2.LINE_AA)\n",
    "                cv2.circle(frame, points[partA], 5, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "                cv2.circle(frame, points[partB], 5, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "        \n",
    "    \n",
    "        scaler = joblib.load(scaler_filename)\n",
    "        temp = np.array(temp).reshape(1,-1)\n",
    "        ges_pred = scaler.transform(temp)\n",
    "        \n",
    "        model_loaded = load_model(subsys2_model)\n",
    "        gesture = model_loaded.predict_classes(ges_pred)\n",
    "        \n",
    "        cv2.putText(frame,gesture_types[gesture[0]], (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('Output-Skeleton', frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        print(\"Time Taken for frame = {}\".format(time.time() - t))\n",
    "        print(num_frames)\n",
    "        vid_writer.write(frame)\n",
    "\n",
    "vid_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
